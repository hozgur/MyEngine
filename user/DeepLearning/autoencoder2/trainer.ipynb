{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, inChannels=3, chunkSizeX = 64,chunkSizeY = 64):\n",
    "        super(CNN, self).__init__()\n",
    "        self.chunkSizeX = chunkSizeX\n",
    "        self.chunkSizeY = chunkSizeY\n",
    "        self.intermediateSize = 1024\n",
    "        self.transfer = nn.Sigmoid()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=3*chunkSizeX*chunkSizeY, out_features=self.intermediateSize),\n",
    "            nn.ReLU(),\n",
    "        )                \n",
    "        self.flatten = nn.Flatten()  \n",
    "        self.unflatten  = nn.Unflatten(1, (3,chunkSizeY,chunkSizeX))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.intermediateSize, out_features=3*chunkSizeX*chunkSizeY),\n",
    "            self.transfer,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.flatten(x)\n",
    "        out = self.encoder(out)\n",
    "        out = self.decoder(out)\n",
    "        out = self.unflatten(out)\n",
    "        return out\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Tensor Shape: torch.Size([2048, 4096, 3])\n",
      "Image size: 4096x2048\n",
      "Remains: 0 - 0\n",
      "trainImageTensor shape after crop=  torch.Size([2048, 4096, 3])\n",
      "New Image size: 4096x2048\n",
      "xChunks: 32\n",
      "yChunks: 16\n",
      "Chunks: 512\n",
      "torch.Size([512, 128, 128, 3])\n",
      "chunk shape =  torch.Size([128, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "# Create Data Set\n",
    "trainImage = Image.open(\"./test-cropped.png\")\n",
    "chunkSizeX = model.chunkSizeX\n",
    "chunkSizeY = model.chunkSizeY\n",
    "trainImageTensor = torch.tensor(np.array(trainImage)).to(device)/255.\n",
    "print(\"Image Tensor Shape: \" + str(trainImageTensor.shape))\n",
    "width = trainImageTensor.shape[1]\n",
    "height = trainImageTensor.shape[0]\n",
    "print(\"Image size: \" + str(width) + \"x\" + str(height))\n",
    "xRemain = width % chunkSizeX\n",
    "yRemain = height % chunkSizeY\n",
    "print(\"Remains: \" + str(xRemain) + \" - \" + str(yRemain))\n",
    "trainImageTensor = trainImageTensor[0:height-yRemain, 0:width-xRemain,:]\n",
    "width = trainImageTensor.shape[1]\n",
    "height = trainImageTensor.shape[0]\n",
    "print(\"trainImageTensor shape after crop= \",trainImageTensor.shape)\n",
    "print(\"New Image size: \" + str(width) + \"x\" + str(height))\n",
    "\n",
    "xChunks = int((width)/chunkSizeX)\n",
    "yChunks = int((height)/chunkSizeY)\n",
    "chunks = xChunks * yChunks\n",
    "print(\"xChunks: \" + str(xChunks))\n",
    "print(\"yChunks: \" + str(yChunks))\n",
    "print(\"Chunks: \" + str(chunks))\n",
    "stride4 = 1\n",
    "stride3 = 3\n",
    "stride2 = width*3\n",
    "stride1 = chunkSizeX\n",
    "stride0 = chunkSizeY*width*3\n",
    "trainImageChunks = trainImageTensor.as_strided((yChunks, xChunks,chunkSizeY, chunkSizeX,3), (stride0,stride1, stride2, stride3, stride4)).flatten(0,1)\n",
    "print(trainImageChunks.shape)\n",
    "print(\"chunk shape = \",trainImageChunks[0].shape)\n",
    "\n",
    "# for i in range(0, 32):\n",
    "#     chunk = trainImageChunks[8192-32+i].cpu()\n",
    "#     img = Image.fromarray(np.uint8(chunk*255)).save(\"./images/test-out_\"+str(i)+\".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchCount =  16\n",
      "Epoch: 1 Loss: 0.17043165862560272\n",
      "Epoch: 2 Loss: 0.1704246699810028\n",
      "Epoch: 3 Loss: 0.1704176664352417\n",
      "Epoch: 4 Loss: 0.17041067779064178\n",
      "Epoch: 5 Loss: 0.1704036444425583\n",
      "Epoch: 6 Loss: 0.17039665579795837\n",
      "Epoch: 7 Loss: 0.17038965225219727\n",
      "Epoch: 8 Loss: 0.17038266360759735\n",
      "Epoch: 9 Loss: 0.17037567496299744\n",
      "Epoch: 10 Loss: 0.17036867141723633\n",
      "Epoch: 11 Loss: 0.17036166787147522\n",
      "Epoch: 12 Loss: 0.1703546941280365\n",
      "Epoch: 13 Loss: 0.1703476756811142\n",
      "Epoch: 14 Loss: 0.17034070193767548\n",
      "Epoch: 15 Loss: 0.17033368349075317\n",
      "Epoch: 16 Loss: 0.17032670974731445\n",
      "Epoch: 17 Loss: 0.17031972110271454\n",
      "Epoch: 18 Loss: 0.17031273245811462\n",
      "Epoch: 19 Loss: 0.17030572891235352\n",
      "Epoch: 20 Loss: 0.1702987402677536\n",
      "Epoch: 21 Loss: 0.1702917516231537\n",
      "Epoch: 22 Loss: 0.17028474807739258\n",
      "Epoch: 23 Loss: 0.17027777433395386\n",
      "Epoch: 24 Loss: 0.17027080059051514\n",
      "Epoch: 25 Loss: 0.17026381194591522\n",
      "Epoch: 26 Loss: 0.1702568233013153\n",
      "Epoch: 27 Loss: 0.1702498495578766\n",
      "Epoch: 28 Loss: 0.17024287581443787\n",
      "Epoch: 29 Loss: 0.17023587226867676\n",
      "Epoch: 30 Loss: 0.17022888362407684\n",
      "Epoch: 31 Loss: 0.17022190988063812\n",
      "Epoch: 32 Loss: 0.1702149361371994\n",
      "Epoch: 33 Loss: 0.17020796239376068\n",
      "Epoch: 34 Loss: 0.17020098865032196\n",
      "Epoch: 35 Loss: 0.17019400000572205\n",
      "Epoch: 36 Loss: 0.17018702626228333\n",
      "Epoch: 37 Loss: 0.1701800525188446\n",
      "Epoch: 38 Loss: 0.17017307877540588\n",
      "Epoch: 39 Loss: 0.17016610503196716\n",
      "Epoch: 40 Loss: 0.17015910148620605\n",
      "Epoch: 41 Loss: 0.17015215754508972\n",
      "Epoch: 42 Loss: 0.170145183801651\n",
      "Epoch: 43 Loss: 0.17013821005821228\n",
      "Epoch: 44 Loss: 0.17013123631477356\n",
      "Epoch: 45 Loss: 0.17012426257133484\n",
      "Epoch: 46 Loss: 0.17011728882789612\n",
      "Epoch: 47 Loss: 0.1701103150844574\n",
      "Epoch: 48 Loss: 0.17010335624217987\n",
      "Epoch: 49 Loss: 0.17009638249874115\n",
      "Epoch: 50 Loss: 0.17008940875530243\n",
      "Epoch: 51 Loss: 0.1700824499130249\n",
      "Epoch: 52 Loss: 0.17007547616958618\n",
      "Epoch: 53 Loss: 0.17006850242614746\n",
      "Epoch: 54 Loss: 0.17006155848503113\n",
      "Epoch: 55 Loss: 0.1700545847415924\n",
      "Epoch: 56 Loss: 0.17004764080047607\n",
      "Epoch: 57 Loss: 0.17004066705703735\n",
      "Epoch: 58 Loss: 0.17003369331359863\n",
      "Epoch: 59 Loss: 0.1700267493724823\n",
      "Epoch: 60 Loss: 0.17001979053020477\n",
      "Epoch: 61 Loss: 0.17001281678676605\n",
      "Epoch: 62 Loss: 0.17000585794448853\n",
      "Epoch: 63 Loss: 0.1699989140033722\n",
      "Epoch: 64 Loss: 0.16999195516109467\n",
      "Epoch: 65 Loss: 0.16998498141765594\n",
      "Epoch: 66 Loss: 0.16997802257537842\n",
      "Epoch: 67 Loss: 0.16997109353542328\n",
      "Epoch: 68 Loss: 0.16996411979198456\n",
      "Epoch: 69 Loss: 0.16995716094970703\n",
      "Epoch: 70 Loss: 0.1699502319097519\n",
      "Epoch: 71 Loss: 0.16994327306747437\n",
      "Epoch: 72 Loss: 0.16993632912635803\n",
      "Epoch: 73 Loss: 0.1699293851852417\n",
      "Epoch: 74 Loss: 0.16992241144180298\n",
      "Epoch: 75 Loss: 0.16991548240184784\n",
      "Epoch: 76 Loss: 0.1699085235595703\n",
      "Epoch: 77 Loss: 0.16990157961845398\n",
      "Epoch: 78 Loss: 0.16989462077617645\n",
      "Epoch: 79 Loss: 0.1698876917362213\n",
      "Epoch: 80 Loss: 0.16988074779510498\n",
      "Epoch: 81 Loss: 0.16987380385398865\n",
      "Epoch: 82 Loss: 0.16986684501171112\n",
      "Epoch: 83 Loss: 0.1698598861694336\n",
      "Epoch: 84 Loss: 0.16985297203063965\n",
      "Epoch: 85 Loss: 0.16984602808952332\n",
      "Epoch: 86 Loss: 0.16983911395072937\n",
      "Epoch: 87 Loss: 0.16983215510845184\n",
      "Epoch: 88 Loss: 0.16982519626617432\n",
      "Epoch: 89 Loss: 0.16981826722621918\n",
      "Epoch: 90 Loss: 0.16981133818626404\n",
      "Epoch: 91 Loss: 0.1698043942451477\n",
      "Epoch: 92 Loss: 0.16979745030403137\n",
      "Epoch: 93 Loss: 0.16979050636291504\n",
      "Epoch: 94 Loss: 0.1697835922241211\n",
      "Epoch: 95 Loss: 0.16977664828300476\n",
      "Epoch: 96 Loss: 0.16976973414421082\n",
      "Epoch: 97 Loss: 0.16976279020309448\n",
      "Epoch: 98 Loss: 0.16975586116313934\n",
      "Epoch: 99 Loss: 0.1697489321231842\n",
      "Epoch: 100 Loss: 0.16974198818206787\n",
      "Epoch: 101 Loss: 0.16973507404327393\n",
      "Epoch: 102 Loss: 0.1697281301021576\n",
      "Epoch: 103 Loss: 0.16972121596336365\n",
      "Epoch: 104 Loss: 0.16971427202224731\n",
      "Epoch: 105 Loss: 0.16970734298229218\n",
      "Epoch: 106 Loss: 0.16970042884349823\n",
      "Epoch: 107 Loss: 0.16969351470470428\n",
      "Epoch: 108 Loss: 0.16968658566474915\n",
      "Epoch: 109 Loss: 0.1696796417236328\n",
      "Epoch: 110 Loss: 0.16967272758483887\n",
      "Epoch: 111 Loss: 0.16966579854488373\n",
      "Epoch: 112 Loss: 0.16965888440608978\n",
      "Epoch: 113 Loss: 0.16965195536613464\n",
      "Epoch: 114 Loss: 0.1696450412273407\n",
      "Epoch: 115 Loss: 0.16963812708854675\n",
      "Epoch: 116 Loss: 0.1696312129497528\n",
      "Epoch: 117 Loss: 0.16962426900863647\n",
      "Epoch: 118 Loss: 0.16961738467216492\n",
      "Epoch: 119 Loss: 0.16961047053337097\n",
      "Epoch: 120 Loss: 0.16960352659225464\n",
      "Epoch: 121 Loss: 0.1695966124534607\n",
      "Epoch: 122 Loss: 0.16958969831466675\n",
      "Epoch: 123 Loss: 0.1695827841758728\n",
      "Epoch: 124 Loss: 0.16957587003707886\n",
      "Epoch: 125 Loss: 0.1695689558982849\n",
      "Epoch: 126 Loss: 0.16956204175949097\n",
      "Epoch: 127 Loss: 0.1695551574230194\n",
      "Epoch: 128 Loss: 0.16954824328422546\n",
      "Epoch: 129 Loss: 0.16954132914543152\n",
      "Epoch: 130 Loss: 0.16953441500663757\n",
      "Epoch: 131 Loss: 0.16952750086784363\n",
      "Epoch: 132 Loss: 0.16952060163021088\n",
      "Epoch: 133 Loss: 0.16951368749141693\n",
      "Epoch: 134 Loss: 0.16950678825378418\n",
      "Epoch: 135 Loss: 0.16949987411499023\n",
      "Epoch: 136 Loss: 0.1694929599761963\n",
      "Epoch: 137 Loss: 0.16948609054088593\n",
      "Epoch: 138 Loss: 0.16947917640209198\n",
      "Epoch: 139 Loss: 0.16947226226329803\n",
      "Epoch: 140 Loss: 0.16946536302566528\n",
      "Epoch: 141 Loss: 0.16945847868919373\n",
      "Epoch: 142 Loss: 0.16945156455039978\n",
      "Epoch: 143 Loss: 0.16944466531276703\n",
      "Epoch: 144 Loss: 0.16943776607513428\n",
      "Epoch: 145 Loss: 0.16943088173866272\n",
      "Epoch: 146 Loss: 0.16942398250102997\n",
      "Epoch: 147 Loss: 0.16941706836223602\n",
      "Epoch: 148 Loss: 0.16941019892692566\n",
      "Epoch: 149 Loss: 0.1694032847881317\n",
      "Epoch: 150 Loss: 0.16939640045166016\n",
      "Epoch: 151 Loss: 0.1693894863128662\n",
      "Epoch: 152 Loss: 0.16938261687755585\n",
      "Epoch: 153 Loss: 0.1693757176399231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"F:\\Python\\Python310\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"F:\\Python\\Python310\\lib\\site-packages\\tensorboard\\summary\\writer\\event_file_writer.py\", line 233, in run\n",
      "    self._record_writer.write(data)\n",
      "  File \"F:\\Python\\Python310\\lib\\site-packages\\tensorboard\\summary\\writer\\record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"F:\\Python\\Python310\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 766, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"F:\\Python\\Python310\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 160, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"F:\\Python\\Python310\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py\", line 164, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: b'runs\\\\Sep28_18-08-55_CAN-PC\\\\events.out.tfevents.1664377735.CAN-PC.5676.2'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 Loss: 0.16936883330345154\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(\"./model1.pth\"))\n",
    "#model.eval()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.1)\n",
    "writer = SummaryWriter()\n",
    "# create the loss function\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "# train the model\n",
    "batchSize = 32\n",
    "epochs = 500\n",
    "batchCount = int(chunks/batchSize)\n",
    "print (\"batchCount = \",batchCount)\n",
    "for epoch in range(1, epochs+1):\n",
    "    for batch in range(0, batchCount):\n",
    "        batchStart = batch * batchSize\n",
    "        batchEnd = batchStart + batchSize\n",
    "        if(batchEnd > chunks):\n",
    "            batchEnd = chunks\n",
    "        batchTensor = trainImageChunks[batchStart:batchEnd].transpose(1,3).transpose(2,3)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batchTensor)\n",
    "        loss = criterion(output, batchTensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"Batch: \" + str(batch) + \" Loss: \" + str(loss.item()))    \n",
    "    print(\"Epoch: \" + str(epoch) + \" Loss: \" + str(loss.item()))    \n",
    "    inputExamples = batchTensor[0:10,:,:,:]\n",
    "    outputExamples = output[0:10,:,:,:]\n",
    "    inputGrid = torchvision.utils.make_grid(inputExamples)\n",
    "    outputGrid = torchvision.utils.make_grid(outputExamples)\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    writer.add_image('input', inputGrid, epoch)\n",
    "    writer.add_image('output', outputGrid, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), \"./model2.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50220305086dde7714464d39eb812e6992001230edb4d06ec4afb2ffb1c28c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
